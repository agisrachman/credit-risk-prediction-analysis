# -*- coding: utf-8 -*-
"""Credit Risk IDX Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BRBFK5nR2peGc7M_cODNK-uWsGwAowiu

# Load Dataset
"""

# Import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from matplotlib import rcParams
rcParams ['figure.figsize'] = (10,7)

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 200)

from google.colab import drive
drive.mount('/content/gdrive')

# Load dataset
df = pd.read_csv('/content/gdrive/MyDrive/loan_data_2007_2014.csv')
print(df.shape) #see the number of rows and columns

# Displays sample data of 3
df.sample(3)

df.drop(columns=['Unnamed: 0'], inplace = True) #it's just a sequence of indexes in the dataset

# For the next process will use the copied data
df_clean = df.copy()

"""# Data Wrangling"""

# Information about the DataFrame
df_clean.info()

# Convert term column to interger
df_clean['term'] = df_clean['term'].apply(lambda term: int(term[:3]))

# Descriptive statistics
df_clean.describe()

# Total value of each column
for col in df:
    print(f'''Value count kolom {col}:''')
    print(df[col].value_counts())
    print('___'*20, '\n')
    print()

"""#Data Cleaning

## Check Duplicated Values
"""

df_clean.duplicated().sum()

"""## Check Missing Values"""

# Total null values
total_null = df_clean.isnull().sum()
percentase_null = df_clean.isnull().sum() * 100/ len(df)
dtype = [df_clean[col].dtype for col in df_clean.columns]
df_missing_value = pd.DataFrame({'total_null': total_null,
                                'data_type': dtype,
                                'percent_missing': percentase_null})
df_missing_value.sort_values('percent_missing', ascending = False,inplace = True)
missing_value = df_missing_value[df_missing_value['percent_missing']>0].reset_index()

fig, ax = plt.subplots(figsize=(15, 15))
plt.title("Missing Value Ratio", fontsize=20, color='black', weight='bold',pad = 50)
sns.barplot(y='index', x='percent_missing', data=missing_value)
plt.bar_label(ax.containers[0], padding=2,fmt='%.2f%%')
plt.xlabel('Percentage (%)', fontsize=14)
plt.ylabel('Feature Name', fontsize=14)
sns.despine()

"""## Handling Missing Values"""

# Drop feature that have more than 20% missing value
col_full_null = df_missing_value.loc[df_missing_value['percent_missing']> 20].index.tolist()
df_clean.drop(columns=col_full_null, inplace = True)

# Feature `tot_coll_amt`,`tot_cur_bal`,`total_rev_hi_lim` replace missing value with "0" because asumption that customer didn't borrowed again
for col in ['tot_coll_amt','tot_cur_bal','total_rev_hi_lim']:
    df_clean[col] = df_clean[col].fillna(0)
    
# Numerical columns replace missing value with "Median"
for col in df_clean.select_dtypes(exclude = 'object'):
    df_clean[col] = df_clean[col].fillna(df_clean[col].median())
df_clean.isnull().sum()

# Categorical columns replace missing value with "Mode"
for col in df_clean.select_dtypes(include = 'object'):
    df_clean[col] = df_clean[col].fillna(df_clean[col].mode().iloc[0])

df_clean.isnull().sum()

"""## Feature target : <mark> is_good"""

df_clean['loan_status'].value_counts()

good_loan = ['Current','Fully Paid','In Grace Period']
df_clean['is_good'] = np.where(df_clean['loan_status'].isin(good_loan),1,0)
df_clean['is_good'].value_counts()

"""## Delete irrelevant columns"""

df_clean.drop(columns=['id','member_id','url','title','addr_state','zip_code','policy_code','application_type','emp_title'], inplace = True)

df_clean.shape

df_clean.head(5)

# For the next process will use the copied data
df_eda = df_clean.copy()

"""# Exploratory Data Analysis

## Univariate Analysis
"""

num = df_eda.select_dtypes(include='number').columns
cat = ['grade','emp_length', 'home_ownership', 'verification_status', 'purpose', 'initial_list_status']

plt.figure(figsize=(24,28))
for i in range(0,len(num)):
    plt.subplot(10,4,i+1)
    sns.kdeplot(x=df_eda[num[i]], palette='viridis', shade=True)
    plt.title(num[i], fontsize=20)
    plt.xlabel(' ')
    plt.tight_layout()

plt.figure(figsize=(20,20))
for i in range(0,len(cat)):
    plt.subplot(3,2,i+1)
    sns.countplot(y=df_eda[cat[i]], orient = 'h',palette='viridis')
    plt.title(cat[i])
    plt.xlabel(' ')
    plt.tight_layout()

"""## Multivariate Analysis"""

plt.figure(figsize=(24,24))
sns.heatmap(df_eda.corr(),annot=True,fmt='.3f')

df_eda.sample(3)

"""# Pre Processing

## Label Encoding
"""

mapping_grade = {'A':'0',
                 'B':'1',
                 'C':'2',
                 'D':'3',
                 'E':'4',
                 'F':'5',
                 'G':'6'}


df_eda['grade'] = df_eda['grade'].map(mapping_grade)

"""## One Hot Encoding"""

for cat in ['home_ownership', 'emp_length']:
    onehots = pd.get_dummies(df[cat], prefix=cat)
    df_eda = df_eda.join(onehots)

df_eda.sample(2)

df_modeling = df_eda.copy()

"""# Modeling"""

#Feature
X = df_modeling[['home_ownership_ANY','home_ownership_MORTGAGE','home_ownership_NONE','home_ownership_OTHER','home_ownership_OWN',
        'home_ownership_RENT','emp_length_1 year','emp_length_10+ years','emp_length_2 years','emp_length_3 years',
        'emp_length_4 years','emp_length_5 years','emp_length_6 years','emp_length_7 years','emp_length_8 years','emp_length_9 years',
        'emp_length_< 1 year','grade','loan_amnt','dti','int_rate','open_acc','total_pymnt','revol_bal','revol_util']]

#Target
y = df_modeling['is_good']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print('Data Train sebanyak :', X_train.shape)
print('Data Test sebanyak :', X_test.shape)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_validate

def eval_classification(model):
    y_pred = model.predict(X_test)
    y_pred_train = model.predict(X_train)
    y_pred_proba = model.predict_proba(X_test)
    y_pred_proba_train = model.predict_proba(X_train)
    
    print("Accuracy (Train Set): %.2f" % accuracy_score(y_train, y_pred_train))
    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, y_pred))
    print("Precision (Train Set): %.2f" % precision_score(y_train, y_pred_train))
    print("Precision (Test Set): %.2f" % precision_score(y_test, y_pred))
    print("Recall (Train Set): %.2f" % recall_score(y_train, y_pred_train))
    print("Recall (Test Set): %.2f" % recall_score(y_test, y_pred))
    print("F1-Score (Train Set): %.2f" % f1_score(y_train, y_pred_train))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, y_pred))
    print("roc_auc (train-proba): %.2f" % roc_auc_score(y_train, y_pred_proba_train[:, 1]))
    print("roc_auc (test-proba): %.2f" % roc_auc_score(y_test, y_pred_proba[:, 1]))


    score = cross_validate(model, X, y, cv=5, scoring='accuracy', return_train_score=True)
    print('accuracy (crossval train): '+ str(score['train_score'].mean()))
    print('accuracy (crossval test): '+ str(score['test_score'].mean()))
    score = cross_validate(model, X, y, cv=5, scoring='precision', return_train_score=True)
    print('Precision (crossval train): '+ str(score['train_score'].mean()))
    print('Precision (crossval test): '+ str(score['test_score'].mean()))

def show_feature_importance(model):
    feat_importances = pd.Series(model.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()

    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model):
    print(model.best_estimator_.get_params())

import numpy as np
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

def draw_learning_curve(param_values):
    train_scores = []
    test_scores = []

    for c in param_values:
        model = LogisticRegression(penalty='l2', C=c)
        model.fit(X_train, y_train)

        # eval on train
        y_pred_train = model.predict(X_train)
        train_recall = recall_score(y_train, y_pred_train)
        train_scores.append(train_recall)

        # eval on test
        y_pred = model.predict(X_test)
        test_recall = recall_score(y_test, y_pred)
        test_scores.append(test_recall)

        print('param value: ' + str(c) + '; train: ' + str(train_recall) + '; test: '+ str(test_recall))

    plt.plot(param_values, train_scores, label='Train')
    plt.plot(param_values, test_scores, label='Test')
    plt.xlabel('C')
    plt.ylabel('Recall')
    plt.title('Learning Curve - Hyperparameter C - Logistic Regression')
    plt.legend()
    plt.show()

"""## Logistics Regression"""

from sklearn import metrics
from sklearn.linear_model import LogisticRegression

logreg=LogisticRegression()
logreg.fit(X_train,y_train)
eval_classification(logreg)

y_pred = logreg.predict(X_test)
cm=metrics.confusion_matrix(y_test,y_pred,labels=[1,0])
df_cm = pd.DataFrame(cm,index=[i for i in ['good','bad']],
                     columns=[i for i in ['Predict good','Predict bad']])
plt.figure(figsize=(7,5))
sns.heatmap(df_cm,annot=True,fmt='g')

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
eval_classification(dt)

"""## Adaboost"""

from sklearn.ensemble import AdaBoostClassifier
clf = AdaBoostClassifier(random_state=42)
clf.fit(X_train, y_train)
eval_classification(clf)